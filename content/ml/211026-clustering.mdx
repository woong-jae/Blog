---
title: "Clustering"
date: "2021-10-26"
emoji: "👨‍👩‍👧‍👦"
category: "ml"
---
## Clustering?
클러스터는 객체간 유사성과 비유사성을 기반으로한 객체의 모음이라고 할 수 있다. 클러스터링은 **유사한 샘플을 모아 같은 그룹**으로 묶는 비지도 학습이다.

클러스터링은 고객이 구매한 것과 활동에 따라서 다른 것들을 추천하거나 특이점(outlier)를 발견하는 데이터 분석에 사용할 수 있다. 차원 축소에도 사용할 수 있다.

## K-means
K-means 알고리즘은 가장 간단한 클러스터링 방식이다. **클러스터의 개수 $k$를 정하고, 랜덤으로 선택한 $k$개의 점을 이용해서 클러스터를 알맞게 변화**시키면서 클러스터를 찾는 방법이다.

전체 점들에 대해서, 각 점마다 가장 가까운 centroid를 찾아서, 찾은 centroid를 기준으로 클러스터를 만든다. 이렇게 만들어진 클러스터의 경우, 실제 최적의 centroid가 기존의 centroid와 달라지게 되므로 각 클러스터마다 새로 centroid를 찾는다. 
이 과정을 클러스터링한 결과가 달라지지 않을 때까지, 즉, centroid들의 거리합이 줄어들지 않는다면 종료한다.

![k-means](http://stanford.edu/~cpiech/cs221/img/kmeansViz.png)

### 동작 순서

1. 점(centroid)들을 랜덤하게 찍는다.
2. Centroid를 기준으로 인스턴스들을 라벨링하고 centroid를 업데이트한다.
3. 2번을 반복하다가 centroid들의 거리합이 더 줄어들지 않으면 끝낸다.

### 문제점
- 점들이 처음에 어디 찍히는지에 따라서 결과가 달라질 수 있다. 즉, sub-optimal한 결과에만 도달할 수도 있다.
    - 해결책: 여러 솔루션을 비교해 최적의 모델을 찾는다. 하지만, 알고리즘을 여러번 돌리는 것 자체도 오래 걸리기 때문에 문제가 된다.

- **계산 복잡도가 크다**. 각 반복 때마다 전체 데이터셋을 사용하기 떄문에 시간이 오래 걸린다. 전체 데이터 셋을 저장할 메모리도 필요하다.
    - 해결책: 훈련 중 mini-batch를 사용한다.

- 클러스터의 수가 몇개인지 유저가 지정해야한다.

또한 클러스터 간의 사이즈, 밀도가 다르고 원형을 가지지 않으면 잘 작동하지 않는다.

### 클러스터링, 언제 사용할까?
#### Preprocessing(전처리)
지도 학습을 하기 전에 전처리 과정으로 클러스터링을 사용할 수 있다. 차원 축소에 효율적으로 쓰일 수 있다.
#### Semi-supervised Learning(반지도 학습)
라벨링 된 트레이닝 셋을 클러스터링 한 후, 라벨이 없는 다른 트레이닝 셋에 라벨을 달아준 후 학습하는 방법(**label propagation**)이다.

## GMM(Gaussian Mixture Model)
가우시안 혼합 모델은 확률에 기반한 모델이다. **데이터 샘플이 여러 가우시안 분포를 혼합한 모델**일 것이라고 가정한다.

이 모델은 PDF(Probability density function, 확률밀도함수)를 잘찾는 것이 목표다.

확률분포 $P(x)$는 $k$개 가우시안의 선형 결합으로 아래와 같이 일반화해서 표현할 수 있다.
$$
P(x)=\sum_{j=1}^k\pi_jN(x;\mu_j,\Sigma_j)
$$

GM을 찾는 것은 최대 우도를 이용한 최적화 문제로 공식화할 수 있다.
$$
\hat{\varTheta}=\argmax_\varTheta\log P(𝕏|\varTheta)
$$

풀이는 다음과 같이 이루어진다. $\varTheta$(가우시안)를 모르기 때문에 난수로 설정하고 출발한다.
1. 가우시안으로 샘플의 소속 정보 개선(E 단계)
2. 샘플의 소속 정보로 가우시안 개선(M 단계)
3. `1~2` 반복

## Anomaly Detection
이상 탐지는 정상인 것들로부터 변이가 좀 많이 강한 인스턴스를 탐지하는 작업이다.

위조 지폐를 찾거나, 제조업에서 결함이 있는 제품을 탐지하거나, 데이터 셋을 다른 모델로 훈련시키기 전에 'outlier'를 제거하는데 사용할 수 있다.

### 이상 탐지 방법들
#### GM
GMM을 사용했을 때 어떤 인스턴스가 밀도가 낮은 지역에 있다면, 그 인스턴스는 anomaly라고 판단 할 수 있다. 
사용자는 어디까지가 outlier인지, 즉, density threshold를 정의해야 한다.
#### PCA
정상적인 인스턴스에 대해 PCA로 차원축소 후 복원한 것과 anomaly에 대해 PCA로 차원축소 후 복구한 것과 비교하면 후자가 복원 에러가 훨씬 크다.
이를 통해 이상 탐지를 할 수 있다.
#### Isolation Forest
DT를 사용한다.
1. 각 노드마다 feature를 랜덤으로 선택하고, 데이터셋을 두 개로 나누기 위한 랜덤 threshold 값을 선탁핸다.
2. 데이터셋은 점진적으로 잘게 나눠지고, 각 인스턴스는 다른 인스턴스로부터 고립된다.
3. Anomaly는 보통 다른 인스턴스로부터 멀리 떨어져있기 때문에 평균적으로 보통 인스턴스들보다 적은 단계에서 고립된다.
![isolation forest](https://ars.els-cdn.com/content/image/1-s2.0-S1474034620301105-gr3.jpg)

<br/>

참조:  
<https://www.secmem.org/blog/2019/05/17/clustering/>