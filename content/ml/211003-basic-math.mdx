---
title: "머신러닝 이해를 위한 기본 수학 개념"
date: "2021-10-03"
emoji: "💆‍♂️"
category: "ml"
---
컴퓨터가 좋은 파라미터를 자동으로 찾아내기 위해서는 '무엇이 좋은 파라미터인가'라는 지표를 정의해야 한다. 이러한 지표를 나타내는 함수를 **목적 함수**라고 한다.

기계학습에서 수학은 목적함수를 최적화하는 역할을 한다. 최적화는 학습을 할 때마다 업데이트 하는 것을 말한다.

머신러닝을 배우는 과정에서 어려움을 덜어낼 수 있도록 기본적인 수학 개념을 짚고 가자.
## 선형대수
### 벡터
샘플의 특징을 벡터(feature vector)로 표현한다.

예를 들어, Iris 데이터에 꽃받침의 길이 꽃받침의 너비, 꽃잎의 길이, 꽃잎의 너비라는 4개의 특징이 각각 5.1, 3.5, 1.4, 0.2인 샘플을 아래와 같이 나타낼 수 있다.
$$
x=\begin{pmatrix}
   5.1 \\
   3.5 \\
   1.4 \\
   0.2
\end{pmatrix}
$$
### 행렬
여러 개의 벡터를 담은 것이다. 백터의 배열이라고 볼 수 있다. 텐서(Tensor)라고도 부른다.

Iris 데이터 150개를 행렬로 표현하면 아래와 같이 나타낼 수 잇다.
$$
x=\begin{pmatrix}
   x_{1,1} && x_{1,2} && x_{1,3} && x_{1,4} \\
   x_{2,1} && x_{2,2} && x_{2,3} && x_{2,4} \\
   ... && ... && ... && ... \\
   x_{149,1} && x_{149,2} && x_{149,3} && x_{149,4} \\
   x_{150,1} && x_{150,2} && x_{150,3} && x_{150,4} \\
\end{pmatrix}
$$

전치행렬(Transpose)는 행과 열을 맞바꾼 행렬이다. 표기는 $x^T$로 한다.

## 확률과 통계
### 베이즈 정리(조건부 확률)
조건부 확률은 어떤 사건이 만들어 놓은 상황에서, 그 사건이 일어난 후 앞으로 일어나게 될 다른 사건의 가능성을 구하는 것을 말한다.
$$
P(y,x)=P(x|y)P(y)=P(x,y)=P(y|x)P(x)
$$
$$
-> P(y,x)={\frac {P(x|y)P(y)} {P(x)}}
$$
### 정보이론
"고비 사막에 눈이 왔다"와 "대관령에 눈이 왔다"라는 두 메시지 중 어느 것이 더 많은 정보를 가질까? 정보이론에 따르면 확률이 작을 수록 많은 정보를 가진다.

- 자기 정보: 사건(메시지) $e_i$의 정보량(단위: bit)
$$
h(e_i)=-log_2(e_i)
$$

## 최적화
기계 학습의 최적화는 단지 훈련집합이 주어지고, 훈련집합에 따라 정해지는 목적함수의 최저점을 찾는 것이다. 따라서 데이터로 미분하는 과정(역전파 알고리즘)이 필요하다. 주로 스토캐스틱 경사 하강법(SGD)를 사용한다.

기계 학습이 해야할 일을 식으로 정의하면, $J(\varTheta)$를 최소로 하는 최적해 $\hat{\varTheta}$를 찾는 것, 즉, $\hat{\varTheta}=\argmin_{\substack{\varTheta}}J(\varTheta)$