---
title: "Decision Tree"
date: "2021-10-12"
emoji: "🌲"
category: "ml"
---
## Decision Tree?
결정 트리는 분류와 회귀 작업 그리고 다중출력 작업도 가능한 다재다능한 머신러닝 알고리즘이다.

결정 트리는 스무고개 하듯이 예/아니오 질문을 이어가면 학습을 한다. 다른 말로 하면, 조건 제어문을 포함하는 알고리즘이다.
데이터의 특성에서 추론된 간단한 결정 규칙들을 학습해서 타겟 변수의 값을 예측하게 된다. 지도학습에 해당된다.

![DT](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/CART_tree_titanic_survivors_KOR.png/350px-CART_tree_titanic_survivors_KOR.png)

### 노드의 attribute
- **Samples**: 노드에 해당되는 인스턴스의 수

- **Value**: 각 클래스의 훈련 인스턴스 중 이 노드에 적용되는 것의 수

- **gini**: 불순도를 나타낸다. 어떤 노드에 적용되는 인스턴스들이 모두 같은 클래스에 속하면 노드가 'pure'(gini = 0)하다고 한다.  
    gini impurity $G_i=1- \sum^n_{\substack{k=1}} p_{i,k}^2$

- Another criterion: Entropy
 
정의: Entropy $H$ of $i^{th}$ node
$$
H_i=-\sum^n_{k=1}p_{i,k}\log_2(p_{i,k})
$$
ML에서 entropy는 어떤 집합이 한 클래스의 인스턴스만 가지면 entropy는 0이다. 즉, pure하다.

### 학습 방법: CART 알고리즘
CART(Classification and Regression Tree) 알고리즘은 다음과 같이 동작한다.
1. Feature $k$와 threshold $t_k$를 사용해서 처음에 훈련 세트를 두 부분집합으로 나눈다.  
    이때 CART는 **가장 pure한 부분집합을 만드는 $(k, t_k)$쌍을 찾는다**.

$$
J(k, t_k)=\frac {m_{left}} m G_{left}+\frac {m_{right}} m G_{right}
$$
$G_{left\ or\ right}$: Impurity  
$m_{left\ or\ right}$: Number of instances

2. 두 부분집합으로 나눈 이후 재귀적으로 계속 나눈다.  
    멈추는 조건:  
    - **Maximum depth**에 도달했을 때
    - 두 부분집합으로 나눔으로써 **불순도를 더이상 낮추지 못할 때**

가장 불순도가 낮은 것을 선택하기 때문에 CART 알고리즘은 그리디 알고리즘이다.

### Regularization Hyperparameters
결정 트리는 **훈련 데이터에 대한 제약 사항이 거의 없다**. 하지만 제한을 두지 않으면 트리가 훈련 데이터에 아주 가깝게 맞추려고 하기 때문에 대부분 **과적합이 발생**한다.

이렇게 훈련되기 전에 **파라미터 수가 결정되지 않는 모델을 'nonparametric model'**이라고 한다.

과적합을 막기위해 규제를 도입해서 결정 트리의 훈련 자유도를 제한해야 한다. 사이킷런은 가장 쉽게 제한할 수 있는 maximum depth부터 규제를 위한 여러가지 hyperparameter를 제공한다.
- min_samples_split: 쪼개기 위한 최소 sample의 수
- min_samples_leaf: leaf node가 가져야 할 최소 sample의 수
- max_features: 각 노드를 쪼개기 위해 평가되는 특성의 최대 수

min\_\* hyperparameter를 올리거나 max\_\*를 내릴 수록 모델이 일반화된다.

### DT 회귀
분류에 사용했던 오차함수에서 $G$를 $MSE$로 바꾸면 회귀를 위한 CART 비용함수가 된다.

### 특성
**장점**
- 정말 간단하다. `if-else`로만 구현이 가능하다.
- 데이터 전처리가 많이 필요없다.
- Classification도 되고 Regression도 된다.
- White box: 모델을 쉽게 해석할 수 있다.
- 데이터의 확률 분포가 뭔지 몰라도 적용이 가능하다. 즉, non-parameteric 데이터도 잘 학습한다.

**단점**
- Training set rotation에 민감하다. DT가 직교 결정 경계를 좋아하기 때문이다.
- Training data의 작은 변화에도 민감하게 반응한다.

<br/>

참조:  
핸즈온 머신러닝  