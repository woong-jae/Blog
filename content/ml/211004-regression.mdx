---
title: "Regression - 회귀"
date: "2021-10-04"
emoji: "📈"
category: "ml"
---
## Linear Regression (선형회귀)
### 정의
- 독립 변수: 'x값이 변함에 따라 y값도 변한다'는 이 정의 안에서 독립적으로 변할 수 있는 x값

- 종속 변수: 독립 변수에 따라 종속적으로 변하는 값

선형회귀는 독립 변수 x를 사용해 종속 변수 y의 움직임을 예측하고 설명하는 작업을 말한다.

공부한 시간을 x로 하고 성적을 y로 두고 '얼마만큼 공부했을 때, 어떤 성적을 얻을 수 있을까?'라는 것을 예측하는 것이 회귀다.

### 오차 수정(잘못 그은 선 바로잡기)
오차 수정은 일단 선을 그리고 조금씩 수정해 나가면서 오차가 최소가 될 때까지 이루어진다. 오차는 일반적으로 예측값과 실제값과의 차이를 뜻한다.

오차를 단순히 `예측값 - 실제값`으로 사용하면, 오차를 전부 더한 값이 오차를 대변하지 못할 수도 있다. 예를 들어, 오차가 `[1, -5, 3, 3]`이면 실제로는 오차가 크지만 모두 더하면 2가 된다.

이것을 해결하기 위해 평균 제곱 오차(Mean Squared Error, MSE)를 사용할 수 있다.
$$
{\frac 1 n}\Sigma(\hat{y_i}-y_i)^2
$$
물론 그냥 제곱만 하거나 절대값만 씌어서 사용해도 무관하다. 오차는 정의하기 나름이다.

### 오차 수정하기
$y=ax+b$에서, 적절한 기울기 a를 찾았을 때 오차가 최소화된다. x축을 기울기, y축을 오차로 해서 그래프를 그려보면 2차 함수같은 모양을 얻을 수 있을 것이다. 여기서 오차를 최소화하려면 2차 함수의 기울기가 0이 되는 a를 찾으면 된다.
이것이 오차를 수정하는 기본적인 원리다.
- **경사하강법**  
    오차의 변화에 따라 이차함수 그래프를 만들고 적절한 학습률을 설정해 미분 값이 0인 지점이다. 학습률은 이동 거리(step)를 정해주는 것이다.

## Polynomial Regression (다항회귀)
데이터를 직선으로 표현할 수 없을 수도 있다. 1차식으로는 한계가 있다.

다항회귀는 2개 이상의 여러 feature들이 있을 때, feature들 간의 관계를 어느 정도 찾을 수 있다.

다항회귀에서는 degree에 따라 underfitting과 overfitting이 나타날 수 있다. loss가 어느 정도에서 더 줄어들지 않는다면 underfitting(degree가 너무 작음)을 의심해 볼 수 있다.
Training set은 잘 맞추는데, 새로운 데이터는 잘 못맞추면 overfitting(degree가 너무 큼)을 의심할 수 있다.

### Regularization (규제, 정규화)
Overfitting을 방지하기 위해 규제를 두는 것이다. 특정 환경에 잘 맞았던 것은 좀 떨어지지만, 새로운 환경에서도 어느 정도 맞게 할 수 있다.

가장 간단한 방법은 degree를 낮추는 것이다.

- Ridge Regression  
    Cost function에 규제를 추가하는 방법이다. 오차에 뭔가 더 추가해서 기울기의 변화율을 조절한다.
- Lasso Regression  
    똑같이 뭔가 추가하지만 'Lasso'는 $l_1$norm을 사용하고 'Ridge'는 $l_2$norm을 사용한다.
- Early Stopping  
    중간에 멈추는 것이다. Validation set의 오차가 최소일 때 중단해서 overfitting을 방지한다.

## Logistic Regression
특정 클래스인지 아닌지 분류하는 것이 목표다(binary classifier).

Logistic function은 아래와 같다.
$$
\sigma(t)={\frac 1 {1+e^{-t}}}
$$
함수에서 나오는 값이 0과 1 사이다. 이 값을 확률로 생각해서 0.5 이상이면 어떤 클래스고, 0.5 미만이면 아니라고 판단하는데 사용할 수 있다.

분류기를 써서 판단하기 애매한 구간을 decision boundary라고 한다.

### Softmax Regression
클래스가 3개 이상일 때 multiple classfier를 만들 수 있다.