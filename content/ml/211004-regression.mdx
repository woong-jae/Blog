---
title: "Regression - 회귀"
date: "2021-10-04"
emoji: "📈"
category: "ml"
---
## Linear Regression (선형회귀)
### 정의
- 독립 변수: 'x값이 변함에 따라 y값도 변한다'는 정의 안에서 독립적으로 변할 수 있는 x값

- 종속 변수: 독립 변수에 따라 종속적으로 변하는 값

선형회귀는 독립 변수 x를 사용해 종속 변수 y의 움직임을 예측하고 설명하는 작업을 말한다.

공부한 시간을 x로 하고 성적을 y로 두고 '얼마만큼 공부했을 때, 어떤 성적을 얻을 수 있을까?'라는 것을 예측하는 것이 회귀다.

#### 선형회귀 모델의 예측
$$
\hat{y}=\theta_0+\theta_1x_1+...+\theta_nx_n=\theta^T𝐱
$$

선형 회귀 모델의 훈련은 훈련 세트에 가장 잘 맞도록 모델 파라미터($\theta_j$)를 설정하는 것이다. 이를 위해서 모델이 훈련 데이터에 얼마나 잘 들어맞는지 측정해야 한다. 이때 사용하는 것이 오차다.

### 오차 수정(잘못 그은 선 바로잡기)
오차 수정은 일단 선을 그리고 조금씩 수정해 나가면서 오차가 최소가 될 때까지 이루어진다. 오차는 일반적으로 예측값과 실제값과의 차이를 뜻한다.

오차를 단순히 `예측값 - 실제값`으로 사용하면, 오차를 전부 더한 값이 오차를 대변하지 못할 수도 있다. 예를 들어, 오차가 `[1, -5, 3, 3]`이면 실제로는 오차가 크지만 모두 더하면 2가 된다.

이것을 해결하기 위한 방법중 하나로 평균 제곱 오차(Mean Squared Error, MSE)를 사용할 수 있다.
$$
{\frac 1 n}\Sigma(\hat{y_i}-y_i)^2
$$
물론 그냥 제곱만 하거나 절대값만 씌어서 사용해도 무관하다. 오차는 정의하기 나름이다.

MSE를 쓴다고 가정했을 때, 우리의 목표는 MSE 비용함수 값을 최소화하는 $\theta$를 찾는 것이다. 아래는 MSE 비용함수다.

$$
MSE(𝐗,h_\theta)=\frac 1 m \sum_{i=1}^m(\theta^T𝐱^{(i)}-y^{(i)})^2
$$

비용함수를 최소화하는 방법에는 두 가지가 있다.
#### 정규방정식
바로 결과를 얻을 수 있는 수학적인 공식을 사용하는 방법이다.

$$
\hat{\theta}=(𝐗^T𝐗)^{-1}𝐗^Ty
$$
$\hat{\theta}$ 값이 비용함수를 최소화 하는 값이다.

#### 배치 경사 하강법(Batch Gradient Descent, BGD)
경사 하강법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것이다. 

파라미터 벡터 $\theta$에 대해 비용함수의 현재 그레이디언트를 계산하고, **그레이디언트가 감소하는 방향으로 점진적으로 진행**한다.
그러다 그레이디언트가 0이 되면 최솟값에 도달한 것이다.

경사 하강법에서 중요한 파라미터는 스텝(step)의 크기로, **학습률** 하이퍼파라미터로 결정된다.

학습률이 너무 크면 골짜기를 가로질러 건너편으로 뛰어 이전보다 더 높은 곳으로 올라갈 수도 있고, 너무 작으면 학습하는데 시간이 오래걸리거나 지역 최솟값에 갇힐 수 있다.

경사 하강법 스텝은 다음과 같이 나타낼 수 있다.

$$
\theta^{(next\ step)}=\theta-\eta\nabla_\theta MSE(\theta)
$$

$\eta$가 비용함수의 그레디이언트 벡터가 얼마나 적용될지 결정하는 학습률이 된다.

> BGD의 경우 매 스텝에 전체 훈련 세트를 사용하기 때문에 훈련 세트가 커지면 매우 느려진다.
>
> 이와 반대로 **확률적 경사 하강법(Stochastic GD)**은 매 스텝에서 한 개의 샘플을 무작위로 선택하고, 그 샘플에 대한 그레이디언트를 선택한다.
> 대신, 무작위 값을 사용하기 때문에 BGD보다 훨씬 불안정하다.

> 이 둘을 절충하는 방법이 **미니배치 경사 하강법**이다. 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산한다.

## Polynomial Regression (다항회귀)
데이터를 직선으로 표현할 수 없을 수도 있다. 1차식으로는 한계가 있다.

다항회귀는 2개 이상의 여러 feature들이 있을 때, feature들 간의 관계를 찾는데 사용할 수 있다.

다항회귀에서는 degree에 따라 underfitting과 overfitting이 나타날 수 있다. loss가 어느 정도에서 더 줄어들지 않는다면 underfitting(degree가 너무 작음)을 의심해 볼 수 있다.
Training set은 잘 맞추는데, 새로운 데이터는 잘 못맞추면 overfitting(degree가 너무 큼)을 의심할 수 있다.

### Regularization (규제, 정규화)
Overfitting을 방지하기 위해 규제를 두는 것이다. 특정 환경에 잘 맞았던 것은 좀 떨어지지만, 새로운 환경에서도 어느 정도 맞게하기 위해 규제를 둔다.

가장 간단한 규제 방법은 degree를 낮추는 것이다.

#### Ridge Regression  
Cost function에 규제를 추가하는 방법이다. 오차에 뭔가 더 추가해서 기울기의 변화율을 조절한다.

***릿지 회귀의 비용함수:***
$$
J(\theta)=MSE(\theta)+\alpha\frac 1 2 \sum_{i=1}^n\theta_i^2
$$

$\alpha$ 값이 커지면 모든 가중치가 거의 0에 가까워지고 결국 데이터의 평균을 지나는 수평선이 된다.
#### Lasso Regression  
똑같이 뭔가 추가하지만 'Lasso'는 $l_1$norm을 사용하고 'Ridge'는 $l_2$norm을 사용한다.

#### Early Stopping  
중간에 멈추는 것이다. Validation set의 오차가 최소일 때 중단해서 overfitting을 방지한다.

## Logistic Regression
특정 클래스인지 아닌지 분류하는 것이 목표다(binary classifier). 샘플이 특정 클래스에 속할 확률을 추정하는데 널리 사용된다.

Logistic function은 아래와 같다.
$$
\sigma(t)={\frac 1 {1+e^{-t}}}
$$
함수에서 나오는 값이 0과 1 사이다. 이 값을 확률로 생각해서 0.5 이상이면 어떤 클래스고, 0.5 미만이면 아니라고 판단하는데 사용할 수 있다.

***로지스틱 함수를 사용한 로지스틱 회귀 모델의 확률 추정:***
$$
\hat{p}=\sigma(\theta^T𝐱)
$$

분류기를 써서 판단하기 애매한 구간을 decision boundary라고 한다.

### Softmax Regression
로지스틱 회귀 모델은 여러 개의 이진 분류기를 훈련시켜 연결하지 않고 직접 다중 클래스를 지원하도록 일반화될 수 있다.

샘플 𝐱가 주어지면 먼저 소프트맥스 회귀 모델이 각 클래스 $k$에 대한 점수 $s_k(𝐱)$를 계산하고, 그 점수에 소프트맥스 함수를 적용해서 각 클래스의 확률을 추정한다.
$$
s_k(𝐱)=(\theta^{(k)})^T𝐱
$$
$$
\hat{p_k}=\sigma(s(𝐱))_k
$$

***소프트맥스 회귀 분류기의 예측:***
$$
\hat{y}=\argmax_k\sigma(s(𝐱))_k
$$

<br/>

참조:  
핸즈온 머신러닝