---
title: "Perceptron(퍼셉트론)"
date: "2021-10-26 22:17"
emoji: "🧠"
category: "ml"
---
## Perceptron?
퍼셉트론은 인간의 신경을 구성하는 뉴런에 영감을 받아 만들어진 초기형태의 신경망이다. 뉴런은 신호를 받아들이고, 이 신호가 일정 크기 이상이면 다른 뉴런에 신호를 전달하게 된다.

이와 비슷하게 퍼셉트론은 여러 입력값을 받아 하나의 결과값을 출력한다. 입력과 출력 사이에는 신호에 대한 처리 과정이 있다. 여러 입력을 합쳐주기 위해 가중합을 구하고 그것을 뉴런이 신호의 역치를 판단하듯이 활성함수에 넣어 결과를 출력을 하게된다.

![Perceptron](https://blog.kakaocdn.net/dn/biE5T3/btqB3qPkNmb/ZfUn6Xo2Np6kJutwdKQO6k/img.png)

퍼셉트론은 평면상에서 XOR 문제를 해결할 수 없는 한계를 가진다. 이 문제를 해결하기 위해서는 차원을 고차원으로 변형해야 한다.
![Perceptron limit](https://wikidocs.net/images/page/24958/xorgraphandxorgate.PNG)
이 변형을 다층 퍼셉트론으로 이룰 수 있다.

### 정의
퍼셉트론은 다음과 같이 정의할 수 있다.
$$
y=\tau(𝗐^𝖳𝗑)
$$
가중치의 합을 활성함수에 넣은 형태다. 활성함수가 step function이면 어느정도 이상의 값만 의미를 가지게 하는 것이 threshold를 정하는 것과 같다고 해서 TLU(Threshold Logic Unit)이라고도 한다.

### 학습
퍼셉트론 학습의 목표는 에러를 최소화하게 $w$값을 개선하는 것이다.
$$
w_{i,j}^{(next\ step)}=w_{i,j}+\eta(y_i-\hat{y_j})x_i
$$
- $\eta$: 학습률
- $y_i-\hat{y_j}$: 실제 - 예측
- $x_i$: 입력값

#### 목적함수
목적함수는 다음과 같은 조건을 만족해야 한다.
- $J(𝗐)\ge 0$
- $𝗐$가 최적이면, 즉 모든 샘플을 맞히면 $J(𝗐)=0$이다.
- 틀리는 샘플이 많은 $w$일수록 $J(𝗐)$는 큰 값을 가진다.

목적함수의 정의는 다음과 같다.
$$
J(𝗐)=\sum_{x_k\in Y}-y_k(𝗐^T𝗑_k)
$$

그레이디언트 계산을 위해 가중치 갱신 규칙($\varTheta=\varTheta-\rho g$)을 적용하려면 gradient $g$가 필요하다.
목적함수를 편미분해서 $g$를 얻을 수 있다. 편미분 결과식을 대입하면 아래 식을 얻을 수 있다.

$$
w_i=w_i+\rho\sum_{x_k\in Y}y_kx_{ki}
$$

- 퍼셉트론 학습 알고리즘(배치 버전)
```clike
// 난수를 생성하여 초기해 w를 설정한다.
repeat
    Y=ø// 틀린 샘플 집합
    for j=1 to n
        // 예측값
        // 틀린 샘플을 집합에 추가한다.
    if (Y!=ø)
        for i=0 to d
            // 틀린 샘플에 대해 그레이디언트 계산
until (Y=ø)
```

## MLP(Multi-layer Perceptron)
다층 퍼셉트론은 기존 퍼셉트론의 입력층과 출력층 사이에 중간층, 즉, 은닉층을 더 끼워넣어 만든다. 출력층을 제외한 모든 층은 모두 연결(fully connected)되어 있어야 한다. 은닉층을 엄청 많이 추가하면 DNN(심층 신경망)이 된다.

**은닉층을 통해 원래 특칭 공간을 분류하는 데 훨씬 유리한 새로운 특징 공간**으로 변환함으로써 퍼셉트론의 선형 분류기라는 한계를 개선한다.

MLP는 학습 방법으로 오류 역전파 알고리즘을 사용한다.

![mlp](https://wikidocs.net/images/page/24958/%EC%9E%85%EC%9D%80%EC%B8%B5.PNG)

### 활성함수
신경망은 다양한 활성함수를 사용한다.

![활성함수](https://user-images.githubusercontent.com/33976823/138913360-a50989c6-6c1c-4247-ab25-e119e116faf7.png)

퍼셉트론은 계단함수, 다층 퍼셉트론은 로지스틱 시그모이드와 하이퍼볼릭 탄젠트, 딥러닝은 ReLU를 사용하는 편이다.

## Back-propagation(BP)
역전파 알고리즘은 신경망 학습의 핵심이론이다. 신경망 내부의 가중치는 오차 역전파 방법을 사용해 수정된다.

BP는 간단하게 말하면 경사 하강법인데, 거기에 gradient를 자동으로 계산하는 효과적인 테크닉을 첨가한 방식이다.

오류 역전파 알고리즘은 **출력층의 오류를 역방향으로 전파하며 gradient를 계산하는 알고리즘**이다.

### 목적함수의 정의
> 학습 알고리즘은 은닉층을 하나라고 국한하고 다룬다.

학습의 목표는 모든 샘플을 옳게 분류하는($Y=f(X)$) 함수 $f$를 찾는 일이다.

- MSE로 정의(온라인 모드): $J(\varTheta)=\frac 1 2 \|y-\hat{y}\|_2^2$

### 알고리즘 설계
MSE로 정의한 목적함수를 편미분한 후, 미분 연쇄 법칙(chain rule)을 사용해서 유도하면 아래와 같은 식을 유도할 수 있다.
$$
\delta_k=(y_k-i_k)\tau'(osum_k), 1\le k\le c
$$
여기서 **$osum_k$는 forward 계산했을 때 저장한 것을 사용**할 수 있다.

이 식으로 GD를 다음과 같이 표현할 수 있다.
$$
\frac {\partial J} {\partial u^2_{kj}}=-\delta_kz_j, 0\le j\le p, 1\le k\le c
$$

이 식을 이용해서 출력층의 오류를 역방향으로 전파하며 gradient를 계산한다.

알고리즘을 정리하면 크게 세 단계로 나눌 수 있다.
1. **Forward pass**: 예측값 구하기

2. **Backward pass**: 입력 레이어에 도달할 때까지 역방향으로 가면서 아래 레이어의 각 연결에서 오류 기여도가 얼마나 되는지 측정한다.

3. **Gradient descent**: 가중치 갱신

- MLP 학습을 위한 스토캐스틱 경사 하강법
```clike
// 가중치 행렬 초기화
repeat
    // 훈련집합의 순서를 섞는다.
    for (훈련집합의 샘플 각각에 대해)
        // 바이어스 설정
        // 전방 계산
        // 오류 역전파
        // 가중치 갱신
until (멈춤조건)
```

<br/>

참조:  
<https://wikidocs.net/24958>